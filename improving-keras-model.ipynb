{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7f85da4",
   "metadata": {},
   "source": [
    "# Improving your Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0073024d",
   "metadata": {},
   "source": [
    "- Dataset: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5303f32",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b3e070a-c30c-490a-a9bc-9923543d3413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda install keras -y\n",
    "# ! conda install tensorflow -y\n",
    "# ! conda install xlrd -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "944c96cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3284bef0-302a-451f-b0b3-b120cb524924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset from UCI ML Repository\n",
    "# ! curl -o default.xls https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a586285",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "df = pd.read_excel('data/default.xls', header=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ad685ab-2b46-4f69-acdd-f4dc206d838f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing data\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdbc6b7-b3c0-4545-b83b-2d4d1c496862",
   "metadata": {},
   "source": [
    "Split into input (X) and output (y) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcb0c799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# predictors include all variables but ID and default\n",
    "X = df.drop(['ID', 'default payment next month'], axis=1)\n",
    "# convert target to categorical\n",
    "y = to_categorical(df['default payment next month'])\n",
    "# note that the y-variable is now one-hot encoded\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43ce3a8e-db62-4287-a2eb-c3e5affed60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into 67% for train and 33% for test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0bf8e",
   "metadata": {},
   "source": [
    "# Baseline Model\n",
    "\n",
    "How many layers should the model contain?   \n",
    "* There's a mountain of commentary on the question of hidden layer configuration in NNs (see the insanely thorough and insightful NN FAQ for an excellent summary of that commentary). One issue within this subject on which there is a consensus is the performance difference from adding additional hidden layers: the situations in which performance improves with a second (or third, etc.) hidden layer are very few. **One hidden layer is sufficient for the large majority of problems.**\n",
    "* There are really two decisions that must be made regarding the hidden layers: how many hidden layers to actually have in the neural network and how many neurons will be in each of these layers. \n",
    "* Neural networks with two hidden layers can represent functions with any kind of shape. There is currently no theoretical reason to use neural networks with any more than two hidden layers.\n",
    " - [source](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8380330-dc92-46fb-9dc5-670a49aa1a24",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efea3001-b9bf-48c3-9127-7083f01a8a1a",
   "metadata": {},
   "source": [
    "The number of nodes in the input layer is always determined by number of predictors. The number of neurons comprising that layer is equal to the number of features (columns) in your data. Note: Some NN configurations add one additional node for a bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99113e8b-d816-456d-954f-80b6e06d2707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "# number of nodes in the input layer \n",
    "nodes_input_layer = X_train.shape[1]\n",
    "print(nodes_input_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabe5242-3c80-4be8-bc53-393b48961c16",
   "metadata": {},
   "source": [
    "Like the Input layer, every NN has exactly one output layer. Determining its size (number of neurons) is simple; it is completely determined by the chosen model configuration.\n",
    "* If the NN is a regressor, then the output layer has a single node.\n",
    "\n",
    "* If the NN is a classifier, then it also has a single node unless softmax is used in which case the output layer has one node per class label in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61ad9f00-c8e2-4c7b-83d3-9c136c2ada0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of nodes in output layer\n",
    "nodes_output_layer = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec2387d-56d8-40f6-840f-5f397e2ac918",
   "metadata": {},
   "source": [
    "The number of nodes in the hidden layers is not easy to determine. There is no universal answer for this question yet. Ultimately, the selection of an architecture for your neural network will come down to trial and error.\n",
    "* Using too few neurons in the hidden layers will result in underfitting\n",
    "* Too many neurons in the hidden layers may result in overfitting   \n",
    "\n",
    "There are many rule-of-thumb methods for determining the correct number of neurons to use in the hidden layers, such as the following:\n",
    "\n",
    "*    The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "*    The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "*    The number of hidden neurons should be less than twice the size of the input layer.\n",
    "\n",
    "- [source](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw)\n",
    "* [further reading](https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0615082f-0d14-4ed6-9afd-ebdbdad47759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of nodes in first hidden layer\n",
    "nodes_hidden_layer = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316107fd-490e-4532-8d2d-9673d60e2832",
   "metadata": {},
   "source": [
    "Rules for the activation function\n",
    "\n",
    "Input and Output Layers:\n",
    "* The input layer does not require an activation function.\n",
    "* For regression problems, the output layer does not require an activation function.\n",
    "* For binary classification problems with a single output variable, the activation function should be \"sigmoid\".\n",
    "* For multi-label classification problems with a single output variable, the activation function should be \"softmax\".  \n",
    "\n",
    "Hidden Layers:\n",
    "* The rectified linear activation function, or ReLU activation function, is perhaps the most common function used for hidden layers.\n",
    "* Sigmoid and Tanh used to be popular but were more susceptible to vanishing gradients that prevent deep models from being trained\n",
    "* Recurrent networks still commonly use Tanh or sigmoid activation functions, or even both. \n",
    "\n",
    "\n",
    "Additional reading:\n",
    "* [Jason Brownlee](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/)\n",
    "* [Keras documentation](https://keras.io/api/layers/activations/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "007015fa-7d5d-47bb-8fad-13a9b7ed9b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation function for the hidden layer\n",
    "activation_function_hidden_layer = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce9d3a31-0877-485c-997a-c8d4f2ea43b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation function for the output layer\n",
    "activation_function_output_layer = 'softmax'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11791793-eecf-4742-a7c7-70fa8c96d6c0",
   "metadata": {},
   "source": [
    "Keras has three APIs for models, which are three options for instantiating the model:\n",
    "* The Sequential API is the simplest. It groups a linear stack of layers.\n",
    "* The Functional API is more complex. It groups layers into an object with training and inference features.\n",
    "* The Subclassed API is the most complex but provides more flexibility\n",
    "\n",
    "A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor. A Sequential model is not appropriate when:\n",
    "\n",
    "* Your model has multiple inputs or multiple outputs\n",
    "* Any of your layers has multiple inputs or multiple outputs\n",
    "* You need to do layer sharing\n",
    "* You want non-linear topology (e.g. a residual connection, a multi-branch model)\n",
    "\n",
    "Further reading:\n",
    "* [Keras documentation](https://keras.io/guides/sequential_model/)\n",
    "* [Introduction to Three Keras Model APIs](https://medium.com/analytics-vidhya/beginner-level-introduction-to-three-keras-model-apis-24a45f7af3c9)\n",
    "* [How to Use the Keras Functional API for Deep Learning](https://machinelearningmastery.com/keras-functional-api-deep-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ed8cac3-c22f-4ad3-a6c6-1693f6eec9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-01 21:49:34.411270: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# define the model using the sequential api\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5386acbc-a468-4a5c-80c9-9874eb2b80b5",
   "metadata": {},
   "source": [
    "Model layers can be added one-by-one using the `.add` method.  \"Dense\" is the most common type of layer. The Keras docs define it as \"Just your regular densely-connected NN layer.\"  \n",
    "\n",
    "Other layer types typically included in more complex models:\n",
    "* Activation layer\n",
    "* Embedding layer\n",
    "* Masking layer\n",
    "\n",
    "\n",
    "Further reading:\n",
    "* [Understanding Keras — Dense Layers](https://medium.com/@hunterheidenreich/understanding-keras-dense-layers-2abadff9b990)\n",
    "* [Keras documentation](https://keras.io/api/layers/core_layers/dense/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16e8d9b8-131c-401b-a144-9dc4b11720d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add layers\n",
    "model.add(Dense(nodes_hidden_layer, \n",
    "                activation=activation_function_hidden_layer, \n",
    "                input_shape = (nodes_input_layer,) # note: the final comma is important\n",
    "               )\n",
    "         )\n",
    "model.add(Dense(nodes_output_layer, \n",
    "                activation=activation_function_output_layer )\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcee95b1-ff83-4974-b9e5-ed43b6dbc2c3",
   "metadata": {},
   "source": [
    "## Compile the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93eff2d-af6c-4650-a001-ad283114d363",
   "metadata": {},
   "source": [
    "The purpose of loss functions is to compute the quantity that a model should seek to minimize during training. The \"default\" selections are as follows:\n",
    "* Regression: Mean Squared Error\n",
    "* Classification: [Cross-Entropy](https://machinelearningmastery.com/cross-entropy-for-machine-learning/) - either binary or categorical.\n",
    "\n",
    "Additional possibilities include:\n",
    "* Regression: Root Mean Squared Error, Mean Squared Logarithmic Error, Mean Absolute Error\n",
    "* Binary Classification: Hinge Loss, Squared Hinge Loss\n",
    "* Multi-Class Classification: Sparse Multiclass Cross-Entropy, Kullback Leibler Divergence\n",
    "\n",
    "Further reading: \n",
    "* [How to choose loss functions](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)\n",
    "* [Loss and Loss Functions for Training Deep Learning Neural Networks](https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/)\n",
    "* [Keras Documentation](https://keras.io/api/losses/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4433ed71-f853-4999-9da8-49be7bfefac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss_function='categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3c63c9-baf4-4740-b1ce-ab82ba6b674c",
   "metadata": {},
   "source": [
    "While several possible optimizers are available, the industry has converged on Adam as the standard choice.\n",
    "* The optimizer is a procedure to update network weights iteratively based in training data.\n",
    "* Traditional optimizers were: stochastic gradient descent (SGD), 'AdaGrad', and 'RMSProp' but these are no longer used.\n",
    "* Adam is an optimization algorithm that was developed in 2015 and realizes the benefits of both AdaGrad and RMSProp.\n",
    "\n",
    "Further reading:\n",
    "* [Introduction to the Adam Optimization Algorithm](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
    "* [Keras documentation](https://keras.io/api/optimizers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa969d85-4e45-4b2b-8ef2-47dc08e1b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization algo\n",
    "optimization_algorithm='adam'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f795b03-ab16-4b4e-b159-92efe71440ed",
   "metadata": {},
   "source": [
    "A metric is a function that is used to judge the performance of your model. Metric functions are similar to loss functions, except that the results from evaluating a metric are not used when training the model. It is not required to specify a metric, and (unlike other parameters) it does not change the training or performance.\n",
    "The most common metrics are:\n",
    "* mean squared error (for regression)\n",
    "* accuracy (for classification)\n",
    "\n",
    "Further reading:\n",
    "- [How to use metrics](https://machinelearningmastery.com/custom-metrics-deep-learning-keras-python/#:~:text=Keras%20allows%20you%20to%20list,()%20function%20on%20your%20model.)\n",
    "- [Keras documentation](https://keras.io/api/metrics/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee5294ad-e0d2-4f68-ba87-fa7cc90f402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are two ways to call the built-in metrics\n",
    "list_of_metrics=['accuracy']\n",
    "list_of_metrics=[keras.metrics.Accuracy(), keras.metrics.Precision(), keras.metrics.Recall()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed63b29-703c-4ac5-941f-c5eb3943d32e",
   "metadata": {},
   "source": [
    "the `.compile()` method configures the model for training. It is a preliminary step to `.fit()`. It requires at least three parameters (but several more can be added as options). \n",
    "\n",
    "Some of the parameters:\n",
    "* loss [required]\n",
    "* optimizer [required]\n",
    "* metrics [optional]\n",
    "\n",
    "Additional reading:\n",
    "* [Keras documentation](https://keras.io/api/models/model_training_apis/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdf1cd9c-e0a3-490b-8b78-ca49b9049fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss=loss_function, \n",
    "              optimizer=optimization_algorithm, \n",
    "              metrics=list_of_metrics\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fff239-cd4c-427f-bb22-45895c04ad7c",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ae1cec-7ebc-4b8f-8aaa-9dc60c0c7e25",
   "metadata": {},
   "source": [
    "Two hyperparameters that often confuse beginners are the batch size and number of epochs. They are both integer values and seem to do the same thing.  \n",
    "* Stochastic gradient descent is an iterative learning algorithm that uses a training dataset to update a model.\n",
    "* The batch size is a hyperparameter of gradient descent that controls the number of training samples to work through before the model’s internal parameters are updated.\n",
    "* The number of epochs is a hyperparameter of gradient descent that controls the number of complete passes through the training dataset.\n",
    "* An epoch is comprised of one or more batches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bbc27a-f91a-41c2-9db5-82aeb9a78d63",
   "metadata": {},
   "source": [
    "How large should the epochs be? Typically, a higher number of epochs will result in a more accurate model but with longer training time and a higher risk of overfitting on the training dataset.\n",
    "* The number of epochs is traditionally large, often hundreds or thousands\n",
    "* You may see examples of the number of epochs in the literature and in tutorials set to 10, 100, 500, 1000, and larger.\n",
    "* You can run the algorithm for as long as you like and even stop it using other criteria besides a fixed number of epochs, such as a change (or lack of change) in model error over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48ffdde2-7e78-4bf3-a7bb-5fc4c4a7e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many epochs?\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e933725-044c-47fb-9f53-7116a3207323",
   "metadata": {},
   "source": [
    "How large should the batch size be? \n",
    "* The size of a batch must be more than or equal to one and less than or equal to the number of samples in the training dataset.\n",
    "* Given that very large datasets are often used to train deep learning neural networks, the batch size is rarely set to the size of the training dataset.\n",
    "\n",
    "Smaller batch sizes (like 32) are often used because:\n",
    "\n",
    "* Smaller batch sizes are noisy, offering a regularizing effect and lower generalization error.\n",
    "* Smaller batch sizes make it easier to fit one batch worth of training data in memory (i.e. when using a GPU).\n",
    "* The batch size is often set at something small, and is not tuned by the practitioner. Small batch sizes such as 32 do work well generally.\n",
    "* Popular batch sizes include 32, 64, and 128 samples.\n",
    "\n",
    "Tradeoffs:\n",
    "* The larger the batch, the more accurate the model. \n",
    "* This comes at the cost of having to use the model to make many more predictions before the estimate can be calculated, and in turn, the weights updated.\n",
    "* A smaller batch results is less accurate - but can result in faster learning and sometimes a more robust model.\n",
    "\n",
    "\n",
    "Further reading:\n",
    "* [How to Control the Stability of Training Neural Networks With the Batch Size](https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/)\n",
    "* [Difference Between a Batch and an Epoch in a Neural Network](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)\n",
    "* [How to Configure Batch Size](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406b4f5f-4365-46c2-bee0-d64afaaba39c",
   "metadata": {},
   "source": [
    "What is the relationship between epochs and batch size? \n",
    "\n",
    "* An epoch is comprised of one or more batches.\n",
    "* One training epoch means that the learning algorithm has made one pass through the training dataset\n",
    "* In each epoch, examples are separated into randomly selected “batch size” groups.  \n",
    "\n",
    "For example, suppose we have a dataset with 1000 rows. If we set batch=200 and epochs=10, then the model will pass through the entire dataset 10 times (totalling 10,000 rows). During each epoch, the model will update the weights 5 times (once for every 200 rows) resulting in 50 updates to the weights. (Note: If the dataset does not divide evenly by batch size, it simply means that the final batch has fewer samples than the other batches.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1000be3-3451-4e06-81d3-8cd5c83895af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "batch_size=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcadc6d1-05b6-46d8-af06-e0b3ccd4ce2e",
   "metadata": {},
   "source": [
    "The `.fit()` method trains the model for a fixed number of epochs (iterations on a dataset). It is the most time-consuming step. The only required inputs are the X and y training data, but a number of optional parameters including:\n",
    "\n",
    "* epochs\n",
    "* batch size\n",
    "* validation data\n",
    "* class weight\n",
    "\n",
    "Additional reading:\n",
    "* [Keras documentation](https://keras.io/api/models/model_training_apis/#fit-method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed1f5a05-bf45-4231-a1e4-f0211459fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 'verbose' option simply determines whether you want to show the output while training.\n",
    "show_output = 0\n",
    "show_output = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85bad199-e864-4246-8ebe-59941e0bfc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-01 21:49:34.575025: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-09-01 21:49:34.575440: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2500005000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2010/2010 [==============================] - 2s 816us/step - loss: 1714.5054 - accuracy: 0.6633 - precision: 0.6752 - recall: 0.6752\n",
      "Epoch 2/10\n",
      "2010/2010 [==============================] - 2s 749us/step - loss: 278.4353 - accuracy: 0.6647 - precision: 0.6959 - recall: 0.6959\n",
      "Epoch 3/10\n",
      "2010/2010 [==============================] - 2s 757us/step - loss: 165.3372 - accuracy: 0.6373 - precision: 0.6947 - recall: 0.6947\n",
      "Epoch 4/10\n",
      "2010/2010 [==============================] - 2s 764us/step - loss: 162.3922 - accuracy: 0.6173 - precision: 0.6872 - recall: 0.6872\n",
      "Epoch 5/10\n",
      "2010/2010 [==============================] - 2s 759us/step - loss: 123.9908 - accuracy: 0.6184 - precision: 0.6954 - recall: 0.6954\n",
      "Epoch 6/10\n",
      "2010/2010 [==============================] - 2s 861us/step - loss: 119.4571 - accuracy: 0.6041 - precision: 0.6908 - recall: 0.6908\n",
      "Epoch 7/10\n",
      "2010/2010 [==============================] - 2s 774us/step - loss: 107.4957 - accuracy: 0.5864 - precision: 0.6918 - recall: 0.6918\n",
      "Epoch 8/10\n",
      "2010/2010 [==============================] - 2s 771us/step - loss: 88.5649 - accuracy: 0.5869 - precision: 0.6995 - recall: 0.6995\n",
      "Epoch 9/10\n",
      "2010/2010 [==============================] - 2s 767us/step - loss: 80.5028 - accuracy: 0.5355 - precision: 0.6839 - recall: 0.6839\n",
      "Epoch 10/10\n",
      "2010/2010 [==============================] - 2s 771us/step - loss: 74.2115 - accuracy: 0.4919 - precision: 0.6816 - recall: 0.6816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcf58fa0220>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "          verbose = show_output\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583272b2-096b-4043-881b-5aebf14e143e",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd365f7-2610-46d2-8c2c-4140e2c26d95",
   "metadata": {},
   "source": [
    "Once you fit a deep learning neural network model, you must evaluate its performance on a test dataset.\n",
    "* The Keras deep learning API model is very limited in terms of the metrics that you can use to report the model performance.\n",
    "* It's common to use the scikit-learn metrics API to evaluate a deep learning model.\n",
    "\n",
    "Further Reading:\n",
    "* [Metrics for Keras](https://machinelearningmastery.com/custom-metrics-deep-learning-keras-python/)\n",
    "* [Calculate Precision, Recall, F1, and More for Deep Learning](https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/)\n",
    "* [Keras documentation](https://keras.io/api/metrics/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d96add6e-7c4c-40a1-8cb5-4dea636cbbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make probability predictions with the model (they come in pairs)\n",
    "y_probs = model.predict(X_test)\n",
    "# make class predictions with the model\n",
    "y_preds = (y_probs > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eaf6f343-a623-4d07-bf2b-214289ac7ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76414144"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy calculated using Keras' method\n",
    "metric = tf.keras.metrics.Accuracy()\n",
    "metric.update_state(y_test, y_preds)\n",
    "metric.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05e4a0ac-e117-4626-9984-0497db9301f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76414144"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the built-in metrics with Keras \n",
    "metric = tf.keras.metrics.BinaryAccuracy()\n",
    "metric.update_state(y_test, y_preds)\n",
    "metric.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ce7deb4-b5e1-4018-b8ac-52a2632c9b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76414144"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the built-in metrics with Keras \n",
    "metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "metric.update_state(y_test, y_preds)\n",
    "metric.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b45938df-ab47-490f-ae50-97ae0e8760c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7641414141414141\n",
      "0.5300129237415453\n",
      "0.5807490324522434\n",
      "0.5300129237415453\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using sklearn\n",
    "print('Accuracy: {}'.format(sklearn.metrics.accuracy_score(y_test, y_preds)))\n",
    "print(sklearn.metrics.roc_auc_score(y_test, y_preds))\n",
    "print(sklearn.metrics.precision_score(y_test, y_preds, average='macro'))\n",
    "print(sklearn.metrics.recall_score(y_test, y_preds, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66e59a66-0807-4871-b597-76ba51c9ec47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.95      0.86      7742\n",
      "           1       0.37      0.11      0.18      2158\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      9900\n",
      "   macro avg       0.58      0.53      0.52      9900\n",
      "weighted avg       0.70      0.76      0.71      9900\n",
      " samples avg       0.76      0.76      0.76      9900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using sklearn\n",
    "print(sklearn.metrics.classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d683bd2-5010-4eef-a072-27a7858afabc",
   "metadata": {},
   "source": [
    "## Recap the Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f847536-7308-46bc-84bb-fef17601e3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2010/2010 [==============================] - 2s 841us/step - loss: 7317.0829 - accuracy: 0.5255 - precision: 0.6764 - recall: 0.6764\n",
      "Epoch 2/10\n",
      "2010/2010 [==============================] - 2s 756us/step - loss: 384.3798 - accuracy: 0.6727 - precision: 0.6981 - recall: 0.6981\n",
      "Epoch 3/10\n",
      "2010/2010 [==============================] - 2s 747us/step - loss: 221.8634 - accuracy: 0.6521 - precision: 0.6931 - recall: 0.6931\n",
      "Epoch 4/10\n",
      "2010/2010 [==============================] - 1s 743us/step - loss: 209.2510 - accuracy: 0.6442 - precision: 0.6863 - recall: 0.6863\n",
      "Epoch 5/10\n",
      "2010/2010 [==============================] - 2s 752us/step - loss: 171.8998 - accuracy: 0.6334 - precision: 0.6870 - recall: 0.6870\n",
      "Epoch 6/10\n",
      "2010/2010 [==============================] - 2s 756us/step - loss: 143.5299 - accuracy: 0.6291 - precision: 0.6906 - recall: 0.6906\n",
      "Epoch 7/10\n",
      "2010/2010 [==============================] - 2s 749us/step - loss: 170.8562 - accuracy: 0.6374 - precision: 0.6892 - recall: 0.6892\n",
      "Epoch 8/10\n",
      "2010/2010 [==============================] - 1s 740us/step - loss: 141.2318 - accuracy: 0.6385 - precision: 0.6913 - recall: 0.6913\n",
      "Epoch 9/10\n",
      "2010/2010 [==============================] - 2s 749us/step - loss: 138.6197 - accuracy: 0.6234 - precision: 0.6925 - recall: 0.6925\n",
      "Epoch 10/10\n",
      "2010/2010 [==============================] - 2s 757us/step - loss: 170.0218 - accuracy: 0.6479 - precision: 0.6996 - recall: 0.6995\n",
      "Accuracy: 0.6419191919191919\n"
     ]
    }
   ],
   "source": [
    "# define the model using the sequential api\n",
    "model = Sequential()\n",
    "# add layers\n",
    "model.add(Dense(nodes_hidden_layer, \n",
    "                activation=activation_function_hidden_layer, \n",
    "                input_shape = (nodes_input_layer,) # note: the final comma is important\n",
    "               )\n",
    "         )\n",
    "model.add(Dense(nodes_output_layer, \n",
    "                activation=activation_function_output_layer )\n",
    "         )\n",
    "# compile the model\n",
    "model.compile(loss=loss_function, \n",
    "              optimizer=optimization_algorithm, \n",
    "              metrics=list_of_metrics\n",
    "             )\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "         )\n",
    "# Evaluate\n",
    "y_probs = model.predict(X_test)\n",
    "y_preds = (y_probs > 0.5).astype(int)\n",
    "print('Accuracy: {}'.format(sklearn.metrics.accuracy_score(y_test, y_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566ae898-f11a-4789-a324-713281f54cdb",
   "metadata": {},
   "source": [
    "# Improving the Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4989306-36c3-461e-9ebc-ac13c6021448",
   "metadata": {},
   "source": [
    "## Add more hidden layers\n",
    "\n",
    "Same as the baseline model but with two additional hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "451933a6-e6df-4b32-8ceb-edf304a51bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7820202020202021\n"
     ]
    }
   ],
   "source": [
    "# define the model using the sequential api\n",
    "model = Sequential()\n",
    "# add layers\n",
    "model.add(Dense(nodes_hidden_layer, \n",
    "                activation=activation_function_hidden_layer, \n",
    "                input_shape = (nodes_input_layer,) # note: the final comma is important\n",
    "               )\n",
    "         )\n",
    "model.add(Dense(nodes_output_layer, \n",
    "                activation=activation_function_output_layer )\n",
    "         )\n",
    "# Here we add two more hidden layers\n",
    "model.add(Dense(nodes_output_layer, \n",
    "                activation=activation_function_output_layer )\n",
    "         )\n",
    "model.add(Dense(nodes_output_layer, \n",
    "                activation=activation_function_output_layer )\n",
    "         )\n",
    "# compile the model\n",
    "model.compile(loss=loss_function, \n",
    "              optimizer=optimization_algorithm, \n",
    "              metrics=list_of_metrics\n",
    "             )\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "          verbose=0\n",
    "         )\n",
    "# Evaluate\n",
    "y_probs = model.predict(X_test)\n",
    "y_preds = (y_probs > 0.5).astype(int)\n",
    "print('Accuracy: {}'.format(sklearn.metrics.accuracy_score(y_test, y_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d326c0cc-b5bd-4e10-85a8-533158c4bf65",
   "metadata": {},
   "source": [
    "## Add more epochs\n",
    "Same as the baseline, but longer epochs and larger batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e037580-d83d-41ff-83bf-9ab958005ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 150\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81c45398-ecdd-4349-aff5-e78344121b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7821212121212121\n"
     ]
    }
   ],
   "source": [
    "# define the model using the sequential api\n",
    "model = Sequential()\n",
    "# add layers\n",
    "model.add(Dense(nodes_hidden_layer, \n",
    "                activation=activation_function_hidden_layer, \n",
    "                input_shape = (nodes_input_layer,) # note: the final comma is important\n",
    "               )\n",
    "         )\n",
    "model.add(Dense(nodes_output_layer, \n",
    "                activation=activation_function_output_layer )\n",
    "         )\n",
    "# compile the model\n",
    "model.compile(loss=loss_function, \n",
    "              optimizer=optimization_algorithm, \n",
    "              metrics=list_of_metrics\n",
    "             )\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "          verbose=0\n",
    "         )\n",
    "# Evaluate\n",
    "y_probs = model.predict(X_test)\n",
    "y_preds = (y_probs > 0.5).astype(int)\n",
    "print('Accuracy: {}'.format(sklearn.metrics.accuracy_score(y_test, y_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51513c-968e-4d56-9383-c2f712b7df8b",
   "metadata": {},
   "source": [
    "# More epochs and more layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "337c5fe1-fadb-48fe-a6bd-eb30b407e224",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 150\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed272d13-6191-444b-a75b-b3d7c2bf1814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7820202020202021\n"
     ]
    }
   ],
   "source": [
    "# define the model using the sequential api\n",
    "model = Sequential()\n",
    "# add layers\n",
    "model.add(Dense(nodes_hidden_layer, \n",
    "                activation=activation_function_hidden_layer, \n",
    "                input_shape = (nodes_input_layer,) # note: the final comma is important\n",
    "               )\n",
    "         )\n",
    "model.add(Dense(nodes_output_layer, \n",
    "                activation=activation_function_output_layer )\n",
    "         )\n",
    "# Here we add two more hidden layers\n",
    "model.add(Dense(nodes_output_layer, \n",
    "                activation=activation_function_output_layer )\n",
    "         )\n",
    "model.add(Dense(nodes_output_layer, \n",
    "                activation=activation_function_output_layer )\n",
    "         )\n",
    "# compile the model\n",
    "model.compile(loss=loss_function, \n",
    "              optimizer=optimization_algorithm, \n",
    "              metrics=list_of_metrics\n",
    "             )\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "          verbose=0\n",
    "         )\n",
    "# Evaluate\n",
    "y_probs = model.predict(X_test)\n",
    "y_preds = (y_probs > 0.5).astype(int)\n",
    "print('Accuracy: {}'.format(sklearn.metrics.accuracy_score(y_test, y_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f3b473-6917-464b-8876-101dab5cd6d9",
   "metadata": {},
   "source": [
    "## Standardize the Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4bfcbd69-54ec-430b-b183-8f90d9fb6bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e0adf5e-70ce-43e7-9041-b7afa812bfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the predictors\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ff2fb36-552c-4211-a45b-85ea6c9f37eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2010/2010 [==============================] - 2s 946us/step - loss: 0.5464 - accuracy: 3.4928e-05 - precision: 0.7778 - recall: 0.7778\n",
      "Epoch 2/10\n",
      "2010/2010 [==============================] - 2s 761us/step - loss: 0.4563 - accuracy: 5.8913e-06 - precision: 0.8161 - recall: 0.8161\n",
      "Epoch 3/10\n",
      "2010/2010 [==============================] - 2s 760us/step - loss: 0.4538 - accuracy: 6.0641e-05 - precision: 0.8152 - recall: 0.8152\n",
      "Epoch 4/10\n",
      "2010/2010 [==============================] - 2s 762us/step - loss: 0.4533 - accuracy: 2.7954e-05 - precision: 0.8140 - recall: 0.8140\n",
      "Epoch 5/10\n",
      "2010/2010 [==============================] - 2s 761us/step - loss: 0.4541 - accuracy: 1.3230e-05 - precision: 0.8130 - recall: 0.8130\n",
      "Epoch 6/10\n",
      "2010/2010 [==============================] - 2s 764us/step - loss: 0.4470 - accuracy: 3.3516e-05 - precision: 0.8179 - recall: 0.8179\n",
      "Epoch 7/10\n",
      "2010/2010 [==============================] - 2s 767us/step - loss: 0.4441 - accuracy: 2.7689e-05 - precision: 0.8176 - recall: 0.8176\n",
      "Epoch 8/10\n",
      "2010/2010 [==============================] - 2s 768us/step - loss: 0.4367 - accuracy: 1.3590e-04 - precision: 0.8199 - recall: 0.8199\n",
      "Epoch 9/10\n",
      "2010/2010 [==============================] - 2s 773us/step - loss: 0.4419 - accuracy: 6.2402e-05 - precision: 0.8173 - recall: 0.8173\n",
      "Epoch 10/10\n",
      "2010/2010 [==============================] - 2s 815us/step - loss: 0.4398 - accuracy: 1.1640e-05 - precision: 0.8199 - recall: 0.8199\n",
      "Accuracy: 0.8168686868686869\n"
     ]
    }
   ],
   "source": [
    "# define the model using the sequential api\n",
    "model = Sequential()\n",
    "# add layers\n",
    "model.add(Dense(nodes_hidden_layer, \n",
    "                activation=activation_function_hidden_layer, \n",
    "                input_shape = (nodes_input_layer,) # note: the final comma is important\n",
    "               )\n",
    "         )\n",
    "model.add(Dense(nodes_output_layer, \n",
    "                activation=activation_function_output_layer )\n",
    "         )\n",
    "# compile the model\n",
    "model.compile(loss=loss_function, \n",
    "              optimizer=optimization_algorithm, \n",
    "              metrics=list_of_metrics\n",
    "             )\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "         )\n",
    "# Evaluate\n",
    "y_probs = model.predict(X_test)\n",
    "y_preds = (y_probs > 0.5).astype(int)\n",
    "print('Accuracy: {}'.format(sklearn.metrics.accuracy_score(y_test, y_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31af01f-ac1f-4385-b879-4d195289f8e0",
   "metadata": {},
   "source": [
    "## Use weights to balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11a89505-dd03-4d1e-8399-bebfa0e974ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Ratio:  0.2212\n"
     ]
    }
   ],
   "source": [
    "# Calculating default Ratio\n",
    "non_default = len(df[df['default payment next month']==0])\n",
    "default = len(df[df['default payment next month']==1])\n",
    "ratio = float(default/(non_default+default))\n",
    "print('Default Ratio: ', ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8b47d9e-563f-4acb-b4c0-0e3014016890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class weight\n",
    "class_weight = {0:ratio, 1:1-ratio}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16856623-02b9-4383-a637-ee9cfe0a072f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2010/2010 [==============================] - 2s 874us/step - loss: 0.2258 - accuracy: 3.6169e-05 - precision: 0.7763 - recall: 0.7763\n",
      "Epoch 2/10\n",
      "2010/2010 [==============================] - 2s 823us/step - loss: 0.2009 - accuracy: 0.0000e+00 - precision: 0.7689 - recall: 0.7689\n",
      "Epoch 3/10\n",
      "2010/2010 [==============================] - 2s 831us/step - loss: 0.2032 - accuracy: 0.0000e+00 - precision: 0.7521 - recall: 0.7521\n",
      "Epoch 4/10\n",
      "2010/2010 [==============================] - 2s 818us/step - loss: 0.2001 - accuracy: 0.0000e+00 - precision: 0.7527 - recall: 0.7527\n",
      "Epoch 5/10\n",
      "2010/2010 [==============================] - 2s 821us/step - loss: 0.1972 - accuracy: 0.0000e+00 - precision: 0.7572 - recall: 0.7572\n",
      "Epoch 6/10\n",
      "2010/2010 [==============================] - 2s 816us/step - loss: 0.1979 - accuracy: 0.0000e+00 - precision: 0.7534 - recall: 0.7534\n",
      "Epoch 7/10\n",
      "2010/2010 [==============================] - 2s 824us/step - loss: 0.1942 - accuracy: 0.0000e+00 - precision: 0.7671 - recall: 0.7671\n",
      "Epoch 8/10\n",
      "2010/2010 [==============================] - 2s 824us/step - loss: 0.1933 - accuracy: 3.3659e-05 - precision: 0.7688 - recall: 0.7688\n",
      "Epoch 9/10\n",
      "2010/2010 [==============================] - 2s 823us/step - loss: 0.1933 - accuracy: 0.0000e+00 - precision: 0.7665 - recall: 0.7665\n",
      "Epoch 10/10\n",
      "2010/2010 [==============================] - 2s 824us/step - loss: 0.1924 - accuracy: 0.0000e+00 - precision: 0.7720 - recall: 0.7720\n",
      "Accuracy: 0.7335353535353535\n"
     ]
    }
   ],
   "source": [
    "# define the model using the sequential api\n",
    "model = Sequential()\n",
    "# add layers\n",
    "model.add(Dense(nodes_hidden_layer, \n",
    "                activation=activation_function_hidden_layer, \n",
    "                input_shape = (nodes_input_layer,) # note: the final comma is important\n",
    "               )\n",
    "         )\n",
    "model.add(Dense(nodes_output_layer, \n",
    "                activation=activation_function_output_layer )\n",
    "         )\n",
    "# compile the model\n",
    "model.compile(loss=loss_function, \n",
    "              optimizer=optimization_algorithm, \n",
    "              metrics=list_of_metrics\n",
    "             )\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "          class_weight=class_weight\n",
    "         )\n",
    "# Evaluate\n",
    "y_probs = model.predict(X_test)\n",
    "y_preds = (y_probs > 0.5).astype(int)\n",
    "print('Accuracy: {}'.format(sklearn.metrics.accuracy_score(y_test, y_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd0018c-87bc-467e-a746-500e49234739",
   "metadata": {},
   "source": [
    "Note that balancing the dataset reduced the overall accuracy but improved the precision and recall\n",
    "\n",
    "**Baseline Metrics**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00b9d53-671e-4423-a8db-c34988dbb6d1",
   "metadata": {},
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.79      0.95      0.86      7742\n",
    "           1       0.37      0.11      0.18      2158"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a1c225-1008-4453-abe7-995e8eb38e60",
   "metadata": {},
   "source": [
    "    micro avg       0.76      0.76      0.76      9900\n",
    "    macro avg       0.58      0.53      0.52      9900\n",
    "    wghtd avg       0.70      0.76      0.71      9900 \n",
    "    sampl avg       0.76      0.76      0.76      9900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0609929-b12c-4c16-8f3c-0c0cf5de5c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.76      0.82      7742\n",
      "           1       0.43      0.64      0.51      2158\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      9900\n",
      "   macro avg       0.65      0.70      0.66      9900\n",
      "weighted avg       0.78      0.73      0.75      9900\n",
      " samples avg       0.73      0.73      0.73      9900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using sklearn\n",
    "print(sklearn.metrics.classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2d044-b1aa-4bc1-95d9-fd6c77b7d242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
